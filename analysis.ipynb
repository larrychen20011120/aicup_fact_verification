{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b76c29-906e-4df2-94cb-6cfbe741c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from dataset import BERTDataset\n",
    "from utils import (\n",
    "    generate_evidence_to_wiki_pages_mapping,\n",
    "    jsonl_dir_to_df,\n",
    "    load_json,\n",
    "    load_model,\n",
    "    save_checkpoint,\n",
    "    set_lr_scheduler,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59ddd9f3-1373-4fb5-b929-d77b773089dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    sup_true, sup_pred = [], []\n",
    "    ref_true, ref_pred = [], []\n",
    "    nei_true, nei_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            y_true.extend(batch[\"labels\"].tolist())\n",
    "\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "            y_pred.extend(list(pred))\n",
    "            \n",
    "            sup_idx = (batch[\"labels\"].detach().cpu().numpy())==0\n",
    "            ref_idx = (batch[\"labels\"].detach().cpu().numpy())==1\n",
    "            nei_idx = (batch[\"labels\"].detach().cpu().numpy())==2\n",
    "            \n",
    "            sup_true.extend(batch[\"labels\"][sup_idx].tolist())\n",
    "            sup_pred.extend(pred[sup_idx].tolist())\n",
    "            ref_true.extend(batch[\"labels\"][ref_idx].tolist())\n",
    "            ref_pred.extend(pred[ref_idx].tolist())\n",
    "            nei_true.extend(batch[\"labels\"][nei_idx].tolist())\n",
    "            nei_pred.extend(pred[nei_idx].tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Supports: {accuracy_score(sup_true, sup_pred)}\")\n",
    "    print(f\"Refutes: {accuracy_score(ref_true, ref_pred)}\")\n",
    "    print(f\"Not Enough Info: {accuracy_score(nei_true, nei_pred)}\")\n",
    "\n",
    "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678607d0-6576-4fb1-a46c-595a2907396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"supports\": 0,\n",
    "    \"refutes\": 1,\n",
    "    \"NOT ENOUGH INFO\": 2,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
    "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
    "\n",
    "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
    "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ca4f4b-f760-4812-b0d6-61068122bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating jsonl files in data/wiki-pages\n",
      "Generate parse mapping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e878b0af950465ea19f76150978f48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=593876), Label(value='0 / 593876')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform to id to evidence_map mapping\n"
     ]
    }
   ],
   "source": [
    "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
    "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
    "del wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d506ff-997e-4100-b9fb-d636398d091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
    "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
    "        item = self.data.iloc[idx]\n",
    "        claim = item[\"claim\"]\n",
    "        evidence = item[\"evidence_list\"]\n",
    "\n",
    "        # In case there are less than topk evidence sentences\n",
    "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
    "        evidence += pad\n",
    "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
    "\n",
    "        concat = self.tokenizer(\n",
    "            concat_claim_evidence,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
    "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
    "\n",
    "        if \"label\" in item:\n",
    "            concat_ten[\"labels\"] = torch.tensor(label)\n",
    "\n",
    "        return concat_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2cc6c6-4fe1-4f8b-ac08-490bc11a53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_topk_evidence(\n",
    "    df: pd.DataFrame,\n",
    "    mapping: dict,\n",
    "    mode: str = \"train\",\n",
    "    topk: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
    "\n",
    "    Note:\n",
    "        After extraction, the dataset will be like this:\n",
    "               id     label         claim                           evidence            evidence_list\n",
    "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
    "        ..    ...       ...            ...                                ...                     ...\n",
    "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
    "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
    "\n",
    "        [946 rows x 5 columns]\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset with evidence.\n",
    "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
    "        topk (int, optional): The topk evidence. Defaults to 5.\n",
    "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
    "            If cache is None, return the result directly.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with topk evidence_list.\n",
    "            The `evidence_list` column will be: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
    "    if \"evidence\" in df.columns:\n",
    "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
    "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
    "            if not isinstance(x[0][0], list) else x)\n",
    "\n",
    "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
    "    if mode == \"eval\":\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
    "            mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "            for evi_id, evi_idx in x  # for each evidence list\n",
    "        ][:topk] if isinstance(x, list) else [])\n",
    "        print(df[\"evidence_list\"][:5])\n",
    "    else:\n",
    "        # extract evidence\n",
    "        df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
    "            \" \".join([  # join evidence\n",
    "                mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
    "                for _, _, evi_id, evi_idx in evi_list\n",
    "            ]) if isinstance(evi_list, list) else \"\"\n",
    "            for evi_list in x  # for each evidence list\n",
    "        ][:1] if isinstance(x, list) else [])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e7f1526-4e34-470f-8253-ecadd66c4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DEV_PKL_FILE, \"rb\") as f:\n",
    "    dev_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f296c7-ae99-4ed6-b1bb-dc9152ba6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-chinese\"\n",
    "CKPT_DIR = \"checkpoints/claim_verification/e20_bs32_1e-05_top5\"\n",
    "ckpt_name = \"val_acc=0.6579_model.3000.pt\" \n",
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9b89b12-6b4f-473f-949c-44d8dfe960d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABEL2ID),\n",
    ")\n",
    "model = load_model(model, ckpt_name, CKPT_DIR)\n",
    "model.to(device)\n",
    "val_dataset = AicupTopkEvidenceBERTDataset(\n",
    "    dev_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    ")\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efac7fe6-0e0f-4c2c-a434-3908a013ae1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4235dd6d0c04f44b47ae2204527ab00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports: 0.7798804780876494\n",
      "Refutes: 0.4962292609351433\n",
      "Not Enough Info: 0.6349924585218703\n",
      "val_loss: 1.4145973847336966\n",
      "val_acc: 0.6579399141630902\n"
     ]
    }
   ],
   "source": [
    "val_results = run_evaluation(model, eval_dataloader, device)\n",
    "\n",
    "# log each metric separately to TensorBoard\n",
    "for metric_name, metric_value in val_results.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
