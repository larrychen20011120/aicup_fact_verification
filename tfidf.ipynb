{"cells":[{"cell_type":"markdown","metadata":{"id":"JeQRZTwXwQum"},"source":["# HW3: Document retrieval of the AICUP2023 competition\n","The goal of this homework is to implement a document retrieval system for the AICUP2023 competition using the [`TF-IDF`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) method."]},{"cell_type":"markdown","metadata":{"id":"Bb-asEyMwQuw"},"source":["## What is TF-IDF?\n","- TF: Term Frequency, the occurring time ($n$) of `a term` $t$ appears in `a document` $d$.\n","$$\\textrm{TF}_{t,d} =  \\frac{n_{t,d}}{\\sum_k n_{k,d}}$$\n",", where $\\sum_k n_{k,d}$ indicates summation of all terms in the document $d$.\n","- IDF: Inverse Document Frequency, how frequent `a term` $t$ appears in all documents.\n","    - Document Frequency $\\textrm{DF}_t = \\log\\frac{|\\{d: t\\in d\\}|}{|D|}$, where $|\\{d: t\\in d\\}|$ is the number of documents that `a term` t appears, and $|D|$ indicates total number of documents.\n","    - $\\Rightarrow \\textrm{IDF}_t = \\log\\frac{|D|}{|\\{d: t\\in d\\}|}$\n","- $\\textrm{TFIDF}_{t,d} = \\textrm{TF}_{t,d} \\times \\textrm{IDF}_t$"]},{"cell_type":"markdown","metadata":{"id":"vbv5x2ZLwQuy"},"source":["## What can TF-IDF do?\n","- TF-IDF can be used to transform a document or a sentence into a `vector`.\n","- There are other ways to transform a document or a sentence into a vector, such as [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html), [`BERT`](https://huggingface.co/docs/transformers/training), [`Sentence-BERT`](https://github.com/UKPLab/sentence-transformers), etc.\n","- **This homework only asks you to implement the TF-IDF method.** You can try other methods in your final project."]},{"cell_type":"markdown","metadata":{"id":"0xYlcoQYwQuz"},"source":["## Example of TF-IDF using `scikit-learn`"]},{"cell_type":"markdown","metadata":{"id":"powtmeYawQu0"},"source":["### Example 1: Use [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) + [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n","```python\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.pipeline import Pipeline\n","\n","# Assume we have 4 documents in a list\n","corpus = [\n","    \"this is the first document\",\n","    \"this document is the second document\",\n","    \"and this is the third one\",\n","    \"is this the first document\",\n","]\n","# Assume we want some important words in the vocabulary\n","vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n","pipe = Pipeline(\n","    [\n","        (\"count\", CountVectorizer(vocabulary=vocabulary)),  # Get the count of each word\n","        (\"tfidf\", TfidfTransformer(norm=None)),  # Get the tfidf of each word\n","    ]\n",").fit(corpus)\n","\n","# Print the count of each word\n","print(pipe[\"count\"].transform(corpus).toarray())\n",">>> [[1, 1, 1, 1, 0, 1, 0, 0], # count of the first document\n","     [1, 2, 0, 1, 1, 1, 0, 0], # count of the second document\n","     [1, 0, 0, 1, 0, 1, 1, 1], # count of the third document\n","     [1, 1, 1, 1, 0, 1, 0, 0]] # count of the fourth document\n","\n","# Print the IDF values of each word\n","# Notice!! The shape of the IDF values is (n_features, ).\n","# n_features is the number of words in the vocabulary.\n","print(pipe[\"tfidf\"].idf_)\n",">>> [1.         1.22314355 1.51082562 1.         1.91629073 1.\n"," 1.91629073 1.91629073]\n","\n","# Print the TF-IDF values\n","# Notice!! The shape of the TF-IDF values is (n_documents, n_features).\n","print(pipe.transform(corpus).toarray())\n",">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n","  0.         0.        ]\n"," [1.         2.4462871  0.         1.         1.91629073 1.\n","  0.         0.        ]\n"," [1.         0.         0.         1.         0.         1.\n","  1.91629073 1.91629073]\n"," [1.         1.22314355 1.51082562 1.         0.         1.\n","  0.         0.        ]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"ZABnGtqlwQu2"},"source":["### Example 2: Use [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) alone\n","```python\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Assume we have 4 documents in a list\n","corpus = [\n","    \"this is the first document\",\n","    \"this document is the second document\",\n","    \"and this is the third one\",\n","    \"is this the first document\",\n","]\n","# Assume we want some important words in the vocabulary\n","vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n","vectorizer = TfidfVectorizer(\n","    vocabulary=vocabulary,\n","    norm=None, # without normalization\n",")\n","X = vectorizer.fit_transform(corpus)\n","print(X.toarray())\n",">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n","  0.         0.        ]\n"," [1.         2.4462871  0.         1.         1.91629073 1.\n","  0.         0.        ]\n"," [1.         0.         0.         1.         0.         1.\n","  1.91629073 1.91629073]\n"," [1.         1.22314355 1.51082562 1.         0.         1.\n","  0.         0.        ]]\n","```\n","- Example 2 is the same as Example 1, but we use `TfidfVectorizer` instead of `CountVectorizer` + `TfidfTransformer`."]},{"cell_type":"markdown","metadata":{"id":"2CUY99OEwQu4"},"source":["## Why do we need to transform a document or a sentence into a vector?\n","- Because we can use the `cosine similarity` to measure the similarity between each sentence and a Wikipedia article.\n","- In our AICUP2023 task, we can find the most similar Wikipedia article for each `claim`."]},{"cell_type":"markdown","metadata":{"id":"Q6mLFb5swQu5"},"source":["## HW3 Delivery policies\n","|Rules | Punishment | Note |\n","|-|-|-|\n","| Name the folder as `FDA_HW3_F12345678` and zip it | -5| |\n","| Include `requirements.txt` in the folder | -5 | |\n","| Use ChatGPT to predict the answers (not allowed in HW3) | -20 | You can use ChatGPT to help coding work. |\n","| Results cannot be reproduced by re-running | -10 | We will run your code! |\n","|Explicit rule-based corrections (e.g. predictions[0] = [\"天衛三\"]) | -20 |  |\n","|Copy and paste the code from your classmates | -20 |  |\n","|Modify the `do-not-modify cell` | -20 |  |"]},{"cell_type":"markdown","metadata":{"id":"HRNq1BqVwQu7"},"source":["## HW3 Scoring rules\n","|Rules | Scores|\n","|-|-|\n","| Perform document retrieval with scikit-learn `TF-IDF` by finishing all # TODO (each for +16) | +80 |\n","| Document retrieval Precision >= 0.25 and Recall >= 0.6 (on the training set) | +10 |\n","| Document retrieval Precision >= 0.4 and Recall >= 0.7 (on the training set) | +10 |\n","| (Bonus) Document retrieval Precision >= 0.5 and Recall >= 0.7 (on the training set) | +10 |\n","| (Bonus) Document retrieval Precision >= 0.6 and Recall >= 0.7 (on the training set) | +10 |"]},{"cell_type":"markdown","metadata":{"id":"v23XuzyVwQu8"},"source":["## Before you start\n","1. Join the AICUP2023 competition. Link: https://tbrain.trendmicro.com.tw/Competitions/Details/28\n","2. Download the training data and the Wikipedia data.\n","3. (This code will need memory about 5.7 GB, tested on Google Colab.)\n","\n","### Folder structure\n","- Please follow the following folder structure.\n","```bash\n","FDA_HW3_F12345678/\n","│\n","├── data/ \n","│ ├── public_train.jsonl # Downloaded from the AICUP2023 page\n","│ └── wiki-pages # Downloaded from the AICUP2023 page and zipped\n","│\n","├── FDA2023_HW3.ipynb # This file\n","├── hw3_utils.py\n","└── requirements.txt\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxpJg8vXwQu9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683123772374,"user_tz":-480,"elapsed":11282,"user":{"displayName":"陳冠廷 CHEN, KUAN-TING F74092269","userId":"13871595842407253140"}},"outputId":"11965162-7c6a-4dc5-fa56-e11170c1251e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandarallel in /usr/local/lib/python3.10/dist-packages (1.6.5)\n","Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (1.5.3)\n","Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (0.3.6)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from pandarallel) (5.9.5)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2022.7.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (1.22.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: TCSP in /usr/local/lib/python3.10/dist-packages (0.0.9)\n"]}],"source":["# Uncomment the following lines if you use Google Colab\n","!pip install pandarallel\n","!pip install TCSP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQIfAGWzwQvB"},"outputs":[],"source":["from pathlib import Path\n","from functools import partial\n","import re\n","import numpy as np\n","import pandas as pd\n","import jieba\n","import scipy\n","\n","# use the jieba dictionary\n","jieba.set_dictionary(\"dict.txt.big\")\n","# Download \"dict.txt.big\" from https://github.com/fxsjy/jieba\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from pandarallel import pandarallel\n","# Adjust the number of workers if you want\n","pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n","\n","from tqdm import tqdm\n","tqdm.pandas() # for progress_apply\n","\n","from hw3_utils import (\n","    load_json,\n","    jsonl_dir_to_df,\n","    calculate_precision,\n","    calculate_recall,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgPB5fMAwQvD"},"outputs":[],"source":["# Get the stopwords\n","# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library\n","from TCSP import read_stopwords_list\n","\n","stopwords = read_stopwords_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnUbC6UkwQvE"},"outputs":[],"source":["def tokenize(text: str, stopwords: list) -> str:\n","    \"\"\"This function performs Chinese word segmentation and removes stopwords.\n","\n","    Args:\n","        text (str): claim or wikipedia article\n","        stopwords (list): common words that contribute little to the meaning of a sentence\n","\n","    Returns:\n","        str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n","    \"\"\"\n","    # TODO: Write your code here\n","    tokens = jieba.cut(text, cut_all=False)\n","\n","    return \" \".join([w for w in tokens if w not in stopwords])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51pjBtbKwQvF"},"outputs":[],"source":["def get_pred_docs_sklearn(\n","    claim: str,\n","    tokenizing_method: callable,\n","    vectorizer: TfidfVectorizer,\n","    tf_idf_matrix: scipy.sparse.csr_matrix,\n","    wiki_pages: pd.DataFrame,\n","    topk: int,\n",") -> set:\n","    \n","    tokens = tokenizing_method(claim)\n","    claim_vector = vectorizer.transform([tokens])\n","    # TODO: Write your code here\n","    similarity_scores = cosine_similarity(tf_idf_matrix, claim_vector)\n","\n","    # `similarity_scores` shape: (num_wiki_pages x 1)\n","    similarity_scores = similarity_scores[:, 0]  # flatten the array\n","\n","    # Sort the similarity scores in descending order\n","    # TODO: Write your code here\n","    sorted_indices = np.flip( np.argsort(similarity_scores) ) # flip: ascending -> descending\n","    # TODO: Write your code here\n","    topk_sorted_indices = sorted_indices[:topk]\n","\n","    # Get the wiki page names based on the topk sorted indices \n","    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n","\n","    exact_matchs = []\n","    # You can find the following code in our AICUP2023 baseline.\n","    # Basically, we check if a result is exactly mentioned in the claim.\n","    for result in results:\n","        if (\n","            (result in claim)\n","            or (result in claim.replace(\" \", \"\")) # E.g., MS DOS -> MSDOS\n","            or (result.replace(\"·\", \"\") in claim) # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n","            or (result.replace(\"-\", \"\") in claim) # E.g., X-SAMPA -> XSAMPA\n","        ):\n","            exact_matchs.append(result)\n","        elif \"·\" in result:\n","            splitted = result.split(\"·\") # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n","            for split in splitted:\n","                if split in claim:\n","                    exact_matchs.append(result)\n","                    break\n","\n","    return set(exact_matchs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dipT9kWwQvG"},"outputs":[],"source":["# Helper function (you don't need to modify this)\n","\n","def get_title_from_evidence(evidence):\n","    titles = []\n","    for evidence_set in evidence:\n","        if len(evidence_set) == 4 and evidence_set[2] is None:\n","            return [None]\n","        for evidence_sent in evidence_set:\n","            titles.append(evidence_sent[2])\n","    return list(set(titles))\n","\n","\n","def save_results_to_markdown(results: dict, output_file=\"grid_search_results.md\"):\n","    file_exists = Path(output_file).exists()\n","\n","    with open(output_file, \"a\") as f:\n","        if not file_exists:\n","            f.write(\"# Grid Search Results\\n\\n\")\n","            f.write(\"| Experiment  | F1 Score | Precision | Recall |\\n\")\n","            f.write(\"| ----------- | -------- | --------- | ------ | \\n\")\n","\n","        exp_name = results[\"exp_name\"]\n","        f1 = results[\"f1_score\"]\n","        prec = results[\"precision\"]\n","        recall = results[\"recall\"]\n","        f.write(f\"| {exp_name} | {f1:.4f} | {prec:.4f} | {recall:.4f} |\\n\")\n","    print(f\"Results saved to {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7x212BDVwQvH"},"outputs":[],"source":["# Hyperparameters\n","\n","wiki_path = \"data/wiki-pages\"\n","min_wiki_length = 20\n","num_of_samples = 500\n","topk = 22\n","min_df = 3\n","max_df = 0.75\n","use_idf = True\n","sublinear_tf = True\n","\n","# Set up the experiment name for logging\n","exp_name = (\n","    f\"len{min_wiki_length}_top{topk}_min_df={min_df}_\"\n","    + f\"max_df={max_df}_{num_of_samples}s\"\n",")\n","if sublinear_tf:\n","    exp_name = \"sublinearTF_\" + exp_name\n","if not use_idf:\n","    exp_name = \"no_idf_\" + exp_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce9wv_BSwQvH"},"outputs":[],"source":["# First time running this cell will 34 minutes using Google Colab.\n","\n","wiki_cache = \"wiki\"\n","target_column = \"text\"\n","\n","wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n","if wiki_cache_path.exists():\n","    wiki_pages = pd.read_pickle(wiki_cache_path)\n","else:\n","    # You need to download `wiki-pages.zip` from the AICUP website\n","    wiki_pages = jsonl_dir_to_df(wiki_path)\n","    # wiki_pages are combined into one dataframe, so we need to reset the index\n","    wiki_pages = wiki_pages.reset_index(drop=True)\n","\n","    # tokenize the text and keep the result in a new column `processed_text`\n","    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n","        partial(tokenize, stopwords=stopwords)\n","    )\n","    # save the result to a pickle file\n","    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNjysy07wQvI"},"outputs":[],"source":["# Build the TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(\n","    # TODO: Write your code here\n","    sublinear_tf=sublinear_tf,\n","    max_df=max_df, min_df=min_df, use_idf=use_idf\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OXVN94X7wQvJ"},"outputs":[],"source":["wiki_pages = wiki_pages[\n","    wiki_pages['processed_text'].str.len() > min_wiki_length\n","]\n","corpus = wiki_pages[\"processed_text\"].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwXgJUgWwQvJ"},"outputs":[],"source":["# Start to encode the corpus with TF-IDF\n","X = vectorizer.fit_transform(corpus)\n","\n","# fit_transform will do the following two steps:\n","# 1. fit: learn the vocabulary and idf from the corpus\n","# 2. transform: transform the corpus into a vector space\n","# Note the result is a sparse matrix, which contains lots of zeros for each row."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQdzGQepwQvK"},"outputs":[],"source":["train = load_json(\"data/public_train.jsonl\")[:num_of_samples]\n","train_df = pd.DataFrame(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27pDgUYywQvK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683124193993,"user_tz":-480,"elapsed":315842,"user":{"displayName":"陳冠廷 CHEN, KUAN-TING F74092269","userId":"13871595842407253140"}},"outputId":"34e8aa0e-fbf0-4d10-a46b-bd4555c4123e"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/500 [00:00<?, ?it/s]Building prefix dict from /content/gdrive/MyDrive/FDA_HW3_F74092269/dict.txt.big ...\n","DEBUG:jieba:Building prefix dict from /content/gdrive/MyDrive/FDA_HW3_F74092269/dict.txt.big ...\n","Dumping model to file cache /tmp/jieba.ud0ccc2c917ffe8c5dd093171a51ebbc5.cache\n","DEBUG:jieba:Dumping model to file cache /tmp/jieba.ud0ccc2c917ffe8c5dd093171a51ebbc5.cache\n","Loading model cost 2.073 seconds.\n","DEBUG:jieba:Loading model cost 2.073 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n","100%|██████████| 500/500 [05:15<00:00,  1.59it/s]\n"]}],"source":["# Perform the prediction for document retrieval\n","# If you use Google Colab, do not use parallel_apply due to the memory limit.\n","\n","# train_df[\"predictions\"] = train_df[\"claim\"].parallel_apply(\n","train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n","    partial(\n","        get_pred_docs_sklearn,\n","        tokenizing_method=partial(tokenize, stopwords=stopwords),\n","        vectorizer=vectorizer,\n","        tf_idf_matrix=X,\n","        wiki_pages=wiki_pages,\n","        topk=topk,\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duhifwccwQvL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683124193998,"user_tz":-480,"elapsed":70,"user":{"displayName":"陳冠廷 CHEN, KUAN-TING F74092269","userId":"13871595842407253140"}},"outputId":"65a55870-a99b-40cd-ec8f-02ebdc0aedd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.5142439431913121\n","Recall: 0.7380116959064328\n","Results saved to grid_search_results.md\n"]}],"source":["precision = calculate_precision(train, train_df[\"predictions\"])\n","recall = calculate_recall(train, train_df[\"predictions\"])\n","results = {\n","    \"exp_name\": exp_name,\n","    \"f1_score\": 2.0 * precision * recall / (precision + recall),\n","    \"precision\": precision,\n","    \"recall\": recall,\n","}\n","\n","# This helps you to adjust the hyperparameters\n","save_results_to_markdown(results, output_file=\"grid_search_results.md\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDH_Vr_VwQvM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683126605702,"user_tz":-480,"elapsed":342196,"user":{"displayName":"陳冠廷 CHEN, KUAN-TING F74092269","userId":"13871595842407253140"}},"outputId":"66590889-5b73-4daa-d500-5685f9628aa9"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3942/3942 [40:12<00:00,  1.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Precision: 0.5092696940618847\n","Recall: 0.7036261544920235\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Do not modify this cell.\n","# This cell is for your scores on the training set.\n","\n","train = load_json(\"data/public_train.jsonl\")\n","train_df = pd.DataFrame(train)\n","\n","# Perform the prediction for document retrieval\n","train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n","    partial(\n","        get_pred_docs_sklearn,\n","        tokenizing_method=partial(tokenize, stopwords=stopwords),\n","        vectorizer=vectorizer,\n","        tf_idf_matrix=X,\n","        wiki_pages=wiki_pages,\n","        topk=topk,\n","    )\n",")\n","precision = calculate_precision(train, train_df[\"predictions\"])\n","recall = calculate_recall(train, train_df[\"predictions\"])"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.16 ('hw3')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"7cbde34494a11b1624bcc1eb71dba5ceb4f0ff8d7a6c820b0d6fa32591a5e209"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}